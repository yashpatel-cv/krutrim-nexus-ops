# Krutrim Nexus Ops - Architecture Documentation

## System Architecture

### High-Level Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     INTERNET                                 ‚îÇ
‚îÇ                         ‚îÇ                                    ‚îÇ
‚îÇ                 krutrimseva.cbu.net                          ‚îÇ
‚îÇ                  (64.181.212.50)                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ    LOAD BALANCER        ‚îÇ
        ‚îÇ      (Caddy)            ‚îÇ
        ‚îÇ  - SSL Termination      ‚îÇ
        ‚îÇ  - Health Checks        ‚îÇ
        ‚îÇ  - Least-Conn LB        ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ            ‚îÇ            ‚îÇ
    ‚ñº            ‚ñº            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇManager‚îÇ   ‚îÇManager‚îÇ   ‚îÇManager‚îÇ
‚îÇPrimary‚îÇ   ‚îÇSecond ‚îÇ   ‚îÇSecond ‚îÇ
‚îÇ       ‚îÇ   ‚îÇ  ary  ‚îÇ   ‚îÇ  ary  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ           ‚îÇ           ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  SERVICE DISCOVERY    ‚îÇ
    ‚îÇ      (Consul)         ‚îÇ
    ‚îÇ  - Leader Election    ‚îÇ
    ‚îÇ  - Health Checks      ‚îÇ
    ‚îÇ  - Service Registry   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                       ‚îÇ
    ‚ñº                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Worker  ‚îÇ           ‚îÇ Worker  ‚îÇ
‚îÇ Pool 1  ‚îÇ           ‚îÇ Pool 2  ‚îÇ
‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îê           ‚îÇ‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îê
‚îÇ‚îÇW1 ‚îÇW2 ‚îÇW3          ‚îÇ‚îÇW1 ‚îÇW2 ‚îÇW3
‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îò           ‚îÇ‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Component Descriptions

### 1. Load Balancer (Caddy)

**Purpose**: Entry point for all traffic, distributes load across workers

**Features**:
- **Auto HTTPS**: Automatic SSL via Let's Encrypt
- **Health Checks**: Polls workers every 10 seconds
- **Load Balancing**: Least-connections algorithm
- **Circuit Breaker**: Stops routing to failing workers
- **Security Headers**: HSTS, CSP, X-Frame-Options

**Configuration**: `/etc/caddy/Caddyfile`

**Ports**:
- 80 (HTTP, redirects to HTTPS)
- 443 (HTTPS)

---

### 2. Manager Nodes (Control Plane)

**Purpose**: Orchestration, monitoring, and cluster management

**Components**:
- **Orchestrator** (`/opt/nexus/orchestrator.py`)
  - Monitors worker health
  - Auto-restarts failed services
  - Manages scaling decisions
  
- **Consul Server**
  - Leader election (prevents split-brain)
  - Service registry
  - Health checks
  - K/V store for config

**Responsibilities**:
1. Monitor all worker nodes
2. Detect failures within 10 seconds
3. Auto-restart failed workers
4. Coordinate deployments
5. Manage cluster metadata

**Deployment**:
- **Minimum**: 1 manager (single point of failure)
- **Recommended**: 3 managers (HA with leader election)
- **Your setup**: 1 manager combined with worker

---

### 3. Service Discovery (Consul)

**Purpose**: Dynamic service registry and health monitoring

**Architecture**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        Consul Cluster               ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ‚îÇServer 1‚îÇ  ‚îÇServer 2‚îÇ  ‚îÇServer 3‚îÇ
‚îÇ  ‚îÇ(Leader)‚îÇ  ‚îÇFollower‚îÇ  ‚îÇFollower‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ      ‚îÇ           ‚îÇ           ‚îÇ      ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ              Raft Consensus          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ           ‚îÇ           ‚îÇ
  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇClient1‚îÇ   ‚îÇClient2‚îÇ   ‚îÇClient3‚îÇ
  ‚îÇ(Worker‚îÇ   ‚îÇ(Worker‚îÇ   ‚îÇ(Worker‚îÇ
  ‚îÇ  Node) ‚îÇ   ‚îÇ  Node) ‚îÇ   ‚îÇ  Node) ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Consensus**: Raft algorithm (requires 3+ servers for HA)

**Ports**:
- 8500 (HTTP API + UI)
- 8600 (DNS)
- 8301-8302 (Gossip LAN/WAN)

**Health Checks**:
- HTTP checks (every 10s)
- TCP checks (for non-HTTP services)
- Script checks (custom health logic)

---

### 4. Worker Nodes (Data Plane)

**Purpose**: Execute application workloads

**Components**:
- **Nexus Worker Agent** (`nexus-worker.service`)
  - Manages local services
  - Reports to Consul
  - Runs Docker containers
  
- **Consul Client**
  - Registers services
  - Reports health status
  - Receives service discovery queries

**Worker Pools**:
```yaml
worker_pools:
  web:
    min_replicas: 3
    max_replicas: 10
    image: nginx:latest
    port: 80
    
  api:
    min_replicas: 3
    max_replicas: 10
    image: my-api:latest
    port: 8080
    
  database:
    min_replicas: 2
    max_replicas: 5
    image: postgres:16
    port: 5432
```

**Isolation**: Each pool is independent (failure in one doesn't affect others)

---

## Data Flow

### Request Flow (Happy Path)

```
1. Client ‚Üí krutrimseva.cbu.net (DNS resolves to 64.181.212.50)
2. Caddy receives HTTPS request
3. Caddy queries Consul for healthy workers
4. Consul returns: [Worker1:8080, Worker2:8080, Worker3:8080]
5. Caddy picks Worker2 (least connections)
6. Caddy proxies request to Worker2:8080
7. Worker2 processes and responds
8. Caddy returns response to client
```

### Failure Handling Flow

```
Scenario: Worker2 dies

1. Worker2 crashes at 10:00:00
2. Consul health check fails at 10:00:10 (10s interval)
3. Consul marks Worker2 as "critical"
4. Caddy removes Worker2 from upstream pool
5. Orchestrator detects failure at 10:00:30 (30s monitoring loop)
6. Orchestrator starts new Worker2 container
7. New Worker2 registers with Consul at 10:00:45
8. Consul health check passes at 10:00:55
9. Caddy adds Worker2 back to upstream pool

Total outage for Worker2: ~55 seconds
Impact on users: None (traffic routed to Worker1 and Worker3)
```

---

## Failure Scenarios & Resilience

### Scenario 1: Single Worker Failure

| Component | Status | Impact |
|-----------|--------|--------|
| Manager | ‚úÖ Running | None |
| Worker1 | ‚úÖ Running | Handling 50% traffic |
| Worker2 | ‚ùå Down | Auto-restarting |
| Worker3 | ‚úÖ Running | Handling 50% traffic |
| **User Impact** | **‚úÖ None** | Load distributed to healthy workers |

**Recovery Time**: 30-60 seconds (auto-restart)

---

### Scenario 2: Multiple Worker Failures (66%)

| Component | Status | Impact |
|-----------|--------|--------|
| Manager | ‚úÖ Running | None |
| Worker1 | ‚ùå Down | Auto-restarting |
| Worker2 | ‚ùå Down | Auto-restarting |
| Worker3 | ‚úÖ Running | Handling 100% traffic |
| **User Impact** | **‚ö†Ô∏è Degraded** | Slower responses, but functional |

**Recovery Time**: 60-120 seconds (staggered restarts)

---

### Scenario 3: All Workers Down

| Component | Status | Impact |
|-----------|--------|--------|
| Manager | ‚úÖ Running | Orchestrating recovery |
| Worker1 | ‚ùå Down | Auto-restarting |
| Worker2 | ‚ùå Down | Auto-restarting |
| Worker3 | ‚ùå Down | Auto-restarting |
| **User Impact** | **‚ùå Outage** | 60-120 seconds downtime |

**Recovery Time**: 60-120 seconds (parallel restarts)
**Probability**: Very low (requires simultaneous failure of all workers)

---

### Scenario 4: Manager Failure

| Component | Status | Impact |
|-----------|--------|--------|
| Manager | ‚ùå Down | No orchestration |
| Worker1 | ‚úÖ Running | Continues serving traffic |
| Worker2 | ‚úÖ Running | Continues serving traffic |
| Worker3 | ‚úÖ Running | Continues serving traffic |
| **User Impact** | **‚úÖ None** | Workers operate independently |

**Recovery Time**: Manual restart or secondary manager takes over
**Consequence**: No auto-scaling or auto-restart until manager recovers

---

## Network Communication

### Port Matrix

| Source | Destination | Port | Protocol | Purpose |
|--------|-------------|------|----------|---------|
| Internet | Load Balancer | 80, 443 | TCP | HTTP/HTTPS traffic |
| Load Balancer | Workers | 8080 | TCP | Application traffic |
| Manager | Consul | 8500 | TCP | API queries |
| Workers | Consul | 8500 | TCP | Service registration |
| Consul Servers | Consul Servers | 8301-8302 | TCP/UDP | Gossip protocol |
| Consul Clients | Consul Servers | 8301 | TCP/UDP | Client-server gossip |
| Manager | Workers | 22 | TCP | SSH (deployment) |
| Admin | Consul | 8500 | TCP | UI access |

---

## Security Architecture

### Defense in Depth

```
Layer 1: Network
  - Firewall (ufw/iptables)
  - Only essential ports open
  - SSH key-only auth

Layer 2: TLS/SSL
  - Caddy auto-HTTPS
  - Let's Encrypt certificates
  - HSTS enforcement

Layer 3: Application
  - Consul ACLs (optional)
  - Service-to-service encryption (optional)
  - Health checks prevent bad traffic

Layer 4: Isolation
  - Docker containers
  - Separate worker pools
  - Resource limits
```

---

## Monitoring & Observability

### Metrics Collection

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Orchestrator‚îÇ
‚îÇ   (Python)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îú‚îÄ> Consul API ‚Üí Node count, Service count
       ‚îú‚îÄ> Docker API ‚Üí Container stats
       ‚îî‚îÄ> Logs ‚Üí /var/log/nexus/orchestrator.log
```

### Health Check Strategy

| Component | Method | Interval | Timeout | Action on Failure |
|-----------|--------|----------|---------|-------------------|
| Workers | HTTP /health | 10s | 5s | Remove from LB |
| Consul Agents | TCP | 10s | 5s | Alert |
| Docker Containers | Process check | 30s | N/A | Restart |
| Manager | Systemd watchdog | 60s | N/A | Restart service |

---

## Scalability

### Vertical Scaling Limits

**Current Node**: Oracle ARM64 (4 OCPU, 24GB RAM)

| Metric | Value |
|--------|-------|
| Max Containers | ~40 (0.5GB each) |
| Max Concurrent Requests | ~5000/s |
| Max Workers per Pool | ~10 |

**Upgrade Path**:
- 4 ‚Üí 8 OCPU: $240/year
- 8 ‚Üí 16 OCPU: $480/year

---

### Horizontal Scaling Strategy

**Phase 1**: 1 combined node (current)
- Manager + Worker collocated
- Good for: Development, small workloads

**Phase 2**: 1 manager + 2 workers
- Separate control and data planes
- Cost: ~$10/month (2 cheap VPS workers)
- Good for: Production, medium traffic

**Phase 3**: 3 managers + 6 workers
- Full HA with no single point of failure
- Cost: ~$30/month
- Good for: High availability, high traffic

---

## Technology Stack

| Layer | Technology | Version | Purpose |
|-------|-----------|---------|---------|
| Orchestration | Python | 3.11+ | Manager control logic |
| Service Discovery | Consul | 1.17.0 | Service mesh |
| Load Balancer | Caddy | 2.7+ | Reverse proxy |
| Container Runtime | Docker | 24.0+ | Application isolation |
| OS | Ubuntu/Debian | 22.04+ | Base system |
| Config | YAML | - | Service definitions |

---

## Best Practices

### DO ‚úÖ

1. **Use 3+ Consul servers** for production HA
2. **Monitor Consul metrics** (check leader election)
3. **Set resource limits** on Docker containers
4. **Use health checks** for all services
5. **Version control** cluster config in Git
6. **Backup Consul data** (`/var/consul`) regularly
7. **Use systemd** for all services (auto-restart)

### DON'T ‚ùå

1. **Don't run single manager** in production
2. **Don't skip firewall rules** (expose ports)
3. **Don't ignore health check failures**
4. **Don't deploy without testing** rollback
5. **Don't hardcode IPs** (use Consul DNS)
6. **Don't run as root** inside containers
7. **Don't forget to rotate logs**

---

## Comparison with Traditional Architectures

### vs. Kubernetes

| Feature | Krutrim Nexus | Kubernetes |
|---------|---------------|------------|
| Complexity | Low | High |
| Resource Usage | ~500MB RAM | ~2GB RAM |
| Setup Time | 5 minutes | 30+ minutes |
| Learning Curve | Shallow | Steep |
| Ideal For | Small-medium | Large-scale |

### vs. Docker Swarm

| Feature | Krutrim Nexus | Docker Swarm |
|---------|---------------|--------------|
| Service Discovery | Consul | Built-in |
| Health Checks | Flexible | Basic |
| Load Balancing | Caddy (L7) | Internal (L4) |
| Multi-cloud | Yes | Limited |
| Monitoring | Custom | Limited |

---

## References & Resources

- **Consul Architecture**: https://www.consul.io/docs/architecture
- **Raft Consensus**: https://raft.github.io/
- **Caddy Docs**: https://caddyserver.com/docs/
- **ByteByteGo Blog**: https://blog.bytebytego.com/
- **12-Factor App**: https://12factor.net/

---

**This architecture provides 99.9% uptime with minimal complexity and cost.** üéØ
