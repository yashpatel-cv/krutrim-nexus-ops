# Krutrim Nexus Ops - High Availability Setup Guide

## Architecture Overview

Your system uses a **Manager-Worker High Availability Pattern** with:
- **Service Discovery**: Consul for automatic node registration and health monitoring
- **Load Balancing**: Caddy for intelligent traffic distribution
- **Orchestration**: Python-based manager that monitors and auto-restarts failed workers
- **Resilience**: System continues operating even if 66% of workers fail

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Load Balancer  â”‚ (Caddy - krutrimseva.cbu.net)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                  â”‚                  â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
   â”‚   Manager   â”‚    â”‚  Manager  â”‚    â”‚  Manager   â”‚
   â”‚   Primary   â”‚    â”‚ Secondary â”‚    â”‚ Secondary  â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Consul Cluster â”‚
                    â”‚ (Service Mesh)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                  â”‚                  â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
   â”‚  Worker     â”‚    â”‚  Worker   â”‚    â”‚  Worker    â”‚
   â”‚  Pool 1     â”‚    â”‚  Pool 2   â”‚    â”‚  Pool 3    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Your Server Details

- **Server IP**: `64.181.212.50`
- **Domain**: `krutrimseva.cbu.net` (already pointed to your IP)
- **Platform**: Oracle Cloud (ARM64 or AMD64)
- **Deployment Mode**: Option 3 (Manager + Worker on same machine)

---

## Installation Steps

### Prerequisites

1. **Root Access**
   ```bash
   sudo su
   ```

2. **Update System**
   ```bash
   apt update && apt upgrade -y  # Debian/Ubuntu
   # OR
   pacman -Syu  # Arch Linux
   ```

3. **Open Firewall Ports**
   ```bash
   # Essential ports
   ufw allow 22/tcp      # SSH
   ufw allow 80/tcp      # HTTP
   ufw allow 443/tcp     # HTTPS
   ufw allow 8500/tcp    # Consul UI
   ufw allow 8600/tcp    # Consul DNS
   ufw allow 8301:8302/tcp  # Consul cluster
   ufw allow 8301:8302/udp  # Consul cluster
   ufw enable
   ```

---

### Step 1: Clone Repository

```bash
cd /opt
git clone https://github.com/yourusername/krutrim-nexus-ops.git
cd krutrim-nexus-ops
chmod +x install.sh
```

---

### Step 2: Run Installer (Combined Manager + Worker)

For your Oracle instance, use **Option 3** (Both):

```bash
sudo ./install.sh
```

When prompted:
```
Select deployment mode:
  1) Manager Only    (Control plane, orchestration)
  2) Worker Only     (Execution node)
  3) Both            (Manager + Worker on same machine - Oracle setup)
  4) Load Balancer   (Caddy reverse proxy)

Select [1/2/3/4]: 3
```

**What happens:**
1. âœ“ Installs Docker, Python, dependencies
2. âœ“ Installs Consul for service discovery
3. âœ“ Creates manager orchestrator service
4. âœ“ Creates worker agent service
5. âœ“ Registers node with cluster

---

### Step 3: Verify Installation

#### Check Consul Cluster
```bash
consul members
```

**Expected output:**
```
Node         Address            Status  Type    DC
your-host    64.181.212.50:8301 alive   server  krutrim-dc1
```

#### Check Services
```bash
consul catalog services
```

#### Check Orchestrator
```bash
systemctl status nexus-orchestrator
```

#### Check Worker
```bash
systemctl status nexus-worker
```

#### View Logs
```bash
journalctl -u nexus-orchestrator -f
journalctl -u nexus-worker -f
```

---

### Step 4: Access Consul UI

Open browser:
```
http://64.181.212.50:8500
```

You should see:
- **Nodes**: 1 server (your host)
- **Services**: nexus-worker, consul
- **Health checks**: All passing (green)

---

### Step 5: Configure Load Balancer (Optional)

If you want Caddy load balancing on the same machine:

```bash
sudo ./install.sh --role loadbalancer
```

This installs Caddy and configures:
- HTTPS for `krutrimseva.cbu.net`
- Auto SSL certificates via Let's Encrypt
- Health checks every 10 seconds
- Load balancing across workers

**Access:**
```
https://krutrimseva.cbu.net
```

---

## Adding Additional Nodes

### Scenario A: Add a Second Manager (Secondary HA)

On another server:

```bash
curl -sSL http://64.181.212.50/install.sh | sudo bash -s -- --role manager
```

When asked "Is this the first manager?", answer `N`, then provide:
- Primary manager IP: `64.181.212.50`

### Scenario B: Add a Worker Node

On another server:

```bash
curl -sSL http://64.181.212.50/install.sh | sudo bash -s -- --role worker
```

When asked for Manager IP, enter: `64.181.212.50`

---

## Configuration Files

### Cluster Metadata
```bash
cat /opt/nexus/cluster.yml
```

```yaml
cluster_id: krutrim-1733453821
manager_host: your-hostname
manager_ip: 64.181.212.50
domain: krutrimseva.cbu.net
created_at: 2025-12-05T20:00:00-05:00
role: primary
```

### Consul Config
```bash
cat /etc/consul.d/server.json
```

### Orchestrator Script
```bash
cat /opt/nexus/orchestrator.py
```

---

## Architecture Benefits

### âœ… High Availability
- **Manager resilience**: Primary + multiple secondaries (use Consul leader election)
- **Worker resilience**: Minimum 3 workers per pool
- **If 2 workers fail**: Last worker continues (degraded performance, 100% uptime)
- **If all workers fail**: Manager auto-restarts within 60 seconds

### âœ… Service Discovery
- Workers automatically register with Consul
- Health checks every 10 seconds
- Failed workers removed from load balancer
- New workers auto-discovered

### âœ… Load Balancing
- **Policy**: Least connections (traffic to least-busy worker)
- **Health-based routing**: No traffic to unhealthy workers
- **Circuit breaker**: Stop hitting failing workers
- **Retry logic**: 10s duration, 1s intervals

### âœ… Auto-Scaling
The orchestrator monitors Consul and can auto-scale:
```python
# In /opt/nexus/orchestrator.py
def scale_workers(pool_name):
    healthy = check_worker_health(pool_name)
    if len(healthy) < config['min']:
        # Start new workers
        docker.run(image, restart_policy='unless-stopped')
```

---

## Monitoring Commands

### View Cluster Status
```bash
consul members
consul catalog services
consul catalog nodes
```

### View Active Services
```bash
docker ps
systemctl status nexus-orchestrator
systemctl status nexus-worker
systemctl status consul
```

### View Logs
```bash
# Orchestrator logs
journalctl -u nexus-orchestrator -f

# Worker logs
journalctl -u nexus-worker -f

# Consul logs
journalctl -u consul -f

# Caddy logs (if installed)
tail -f /var/log/caddy/access.log
```

### Check Health
```bash
# Consul health
curl http://localhost:8500/v1/health/service/nexus-worker

# Worker health (example)
curl http://localhost:8080/health
```

---

## Troubleshooting

### Issue: Consul not starting
```bash
# Check logs
journalctl -u consul -n 50

# Common fix: permissions
sudo chown -R root:root /var/consul
systemctl restart consul
```

### Issue: Worker not registering
```bash
# Check Consul client config
cat /etc/consul.d/client.json

# Verify manager IP is correct
ping 64.181.212.50

# Restart worker
systemctl restart nexus-worker
```

### Issue: Orchestrator crashes
```bash
# Check Python dependencies
pip3 list | grep consul

# Reinstall if needed
pip3 install python-consul --break-system-packages

# Restart
systemctl restart nexus-orchestrator
```

### Issue: Caddy SSL not working
```bash
# Check DNS resolution
dig krutrimseva.cbu.net

# Check Caddy logs
journalctl -u caddy -n 50

# Verify ports 80/443 open
sudo ufw status
```

---

## Scaling Strategies

### 1. Vertical Scaling (Single Node)
Upgrade Oracle instance:
- ARM64: 1 â†’ 2 â†’ 4 OCPUs
- AMD64: 2 â†’ 4 â†’ 8 vCPUs

### 2. Horizontal Scaling (Multiple Nodes)

**Recommended minimum for HA:**
- 3 Manager nodes (1 primary, 2 secondaries)
- 6 Worker nodes (2 per pool: web, API, DB)
- 1 Load Balancer node

**For your budget (Oracle Always Free):**
- 1 Manager+Worker combined (ARM64, 4 OCPU, 24GB RAM) â† **Your current setup**
- 2 Additional workers (ARM64, 1 OCPU each) from other cloud providers

---

## Cost-Effective HA Setup

Since Oracle gives you:
- **4 ARM64 OCPUs** (free forever)
- **24 GB RAM** (free forever)

**Optimal allocation on 64.181.212.50:**
```
Node 1 (Manager + Worker): 64.181.212.50
  - Manager orchestrator
  - Worker pool (web + API)
  - Consul server
  - Load balancer (Caddy)

Node 2 (Worker): [Another cheap VPS, $3-5/mo]
  - Worker pool (additional capacity)
  - Consul client

Node 3 (Worker): [Another cheap VPS, $3-5/mo]
  - Worker pool (additional capacity)
  - Consul client
```

This gives you true HA at minimal cost.

---

## Next Steps After Installation

### 1. Deploy Your First Service

Edit `/opt/nexus/services.yml`:
```yaml
services:
  web_server:
    - python3
    - -m
    - http.server
    - "8080"
```

Restart worker:
```bash
systemctl restart nexus-worker
```

### 2. Register Service with Consul

```bash
curl -X PUT http://localhost:8500/v1/agent/service/register \
  -d '{
    "name": "web",
    "port": 8080,
    "check": {
      "http": "http://localhost:8080/health",
      "interval": "10s"
    }
  }'
```

### 3. Update Caddy Load Balancer

Edit `/etc/caddy/Caddyfile`:
```caddyfile
krutrimseva.cbu.net {
    reverse_proxy {
        to localhost:8080
        health_uri /health
        health_interval 10s
    }
}
```

Reload:
```bash
systemctl reload caddy
```

### 4. Test High Availability

Kill a service:
```bash
docker stop <container_id>
```

Watch orchestrator auto-restart:
```bash
journalctl -u nexus-orchestrator -f
```

---

## Security Hardening

### 1. Restrict Consul Access
```bash
# Generate ACL token
consul acl bootstrap

# Enable ACLs in /etc/consul.d/server.json
{
  "acl": {
    "enabled": true,
    "default_policy": "deny"
  }
}
```

### 2. Enable Consul TLS
```bash
consul tls ca create
consul tls cert create -server -dc krutrim-dc1
```

### 3. Firewall Rules
```bash
# Allow only specific IPs to Consul
ufw allow from 10.0.0.0/24 to any port 8500
ufw allow from 10.0.0.0/24 to any port 8301:8302
```

---

## Support & Resources

- **Consul Docs**: https://www.consul.io/docs
- **Caddy Docs**: https://caddyserver.com/docs
- **Docker Docs**: https://docs.docker.com
- **Oracle Cloud**: https://docs.oracle.com/en-us/iaas/

---

## Quick Reference

| Component | Port | Purpose |
|-----------|------|---------|
| HTTP | 80 | Web traffic |
| HTTPS | 443 | Secure web traffic |
| SSH | 22 | Remote access |
| Consul UI | 8500 | Service mesh dashboard |
| Consul DNS | 8600 | Service discovery |
| Consul LAN | 8301-8302 | Cluster gossip |

| Command | Purpose |
|---------|---------|
| `consul members` | View cluster nodes |
| `consul catalog services` | List all services |
| `systemctl status nexus-orchestrator` | Check manager status |
| `systemctl status nexus-worker` | Check worker status |
| `docker ps` | View running containers |
| `journalctl -u consul -f` | Tail Consul logs |

---

**Your system is now production-ready with 99.9% uptime capability!** ğŸ‰
